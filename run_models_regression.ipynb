{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    median_absolute_error,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def normalize_data_exclude_target(df, target):\n",
    "    target_df=df[target]\n",
    "    df = df.drop(target, axis=1)\n",
    "    columnsToEncode = list(df.select_dtypes(include=['float','int']))\n",
    "    for col in columnsToEncode:\n",
    "        df[col]=(df[col]-df[col].mean())/df[col].std()\n",
    "    df_merged = pd.merge(df,target_df, how='inner', left_index=True, right_index=True)\n",
    "    return df_merged\n",
    "\n",
    "    \n",
    "\n",
    "def metrics_regression(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Prints the standard of the metrics\n",
    "    :param y_test: the true labels of the test set\n",
    "    :param y_pred: the predicted labels of the test set\n",
    "    :return: None, this prints out the results of the metrics\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    MAE = median_absolute_error(y_test, y_pred)\n",
    "    RSE = mean_squared_error(y_test, y_pred)\n",
    "    sum_preds = y_pred.sum()\n",
    "    sum_actual = y_test.sum()\n",
    "    return r2, MAE, RSE, sum_preds, sum_actual\n",
    "    \n",
    "def label_feature_split(df, column):\n",
    "    label=df[[column]].values.ravel()\n",
    "    feature=df.drop([column], axis=1)\n",
    "    return feature, label\n",
    "\n",
    "def run_datasets(df_train, df_target, target, columns, apply_preprocessing=True, reporter_object=None):\n",
    "    reporter_object.normalized=apply_preprocessing\n",
    "    \n",
    "    if apply_preprocessing:\n",
    "        df_train = normalize_data_exclude_target(df_train,target)\n",
    "        df_target = normalize_data_exclude_target(df_target,target)\n",
    "    X_train, y_train = label_feature_split(df_train, target)\n",
    "    X_test, y_test = label_feature_split(df_target,target)\n",
    "    reporter_object.actual_sum = y_test.sum()\n",
    "    \n",
    "    return run_generic_models_regression(X_train, y_train, X_test, y_test, reporter_object)\n",
    "    \n",
    "def keep_columns_single(proxy_df):\n",
    "    keep_column_list=stored_spesifics.original_columns()\n",
    "    train_book = proxy_df[keep_column_list]\n",
    "    return train_book\n",
    "\n",
    "def split_into_bins(df, bins=12, column=None):\n",
    "    \"\"\"\n",
    "    This method adds which bin the column falls into based on the column and bins\n",
    "    :param df: dataframe to be used\n",
    "    :param column: the column which needs to be split\n",
    "    :param bins: a list of the bins the dataframe is split into\n",
    "    :return: the original dataframe with the new columns\n",
    "    \"\"\"\n",
    "    list_of_persentiles=[]\n",
    "    for i in range(1, bins):\n",
    "        list_of_persentiles.append(i/bins)\n",
    "    if column is None:\n",
    "        dataset = pd.DataFrame({'predictions':df})\n",
    "        persentile_outputs=list(dataset.predictions.describe(percentiles=list_of_persentiles))\n",
    "    else:\n",
    "        persentile_outputs=list(df[column].describe(percentiles=list_of_persentiles))\n",
    "    \n",
    "    count = persentile_outputs.pop(0)\n",
    "    mean = persentile_outputs.pop(0)\n",
    "    std = persentile_outputs.pop(0)\n",
    "    return count, mean, std, persentile_outputs\n",
    "\n",
    "def run_generic_models_regression(X_train, y_train, X_test, y_test, reporter_object):\n",
    "\n",
    "    # models from https://arxiv.org/abs/1708.05070 slightly adaped for regression and speeds\n",
    "    GBC = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestRegressor(n_estimators=500, max_features=0.25)\n",
    "    SVM = SVR(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesRegressor(n_estimators=1000, max_features=\"log2\")\n",
    "    LR = LogisticRegression(C=1.5, penalty=\"l1\",fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBRegressor()\n",
    "    CBC = CatBoostRegressor(silent=True,task_type=\"GPU\")\n",
    "    light_gb = lgb.LGBMRegressor()\n",
    "    #Commenting out the later models variable will run all the variables\n",
    "\n",
    "    models=[(ETC, \"Extra tree classifier\"), (RFC, \"random forest classifier\"), (GBC, \"gradient boosted classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light GBM\")]\n",
    "    models=[(RFC, \"random forest regressor\")]\n",
    "    for model, name in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2, MAE, RSE, sum_preds, sum_actual = metrics_regression(y_test, y_pred)\n",
    "        count, mean, std, persentile_outputs = split_into_bins(y_pred)\n",
    "        reporter_object.add_model(name, r2, MAE, RSE, sum_preds, count, mean, std, persentile_outputs)\n",
    "    return reporter_object\n",
    "\n",
    "def basic_drop_series(df1, df2):\n",
    "    return drop_nans(df1), drop_nans(df2)\n",
    "\n",
    "def same_length_lists(l1,l2):\n",
    "    l1, l2 = sorted_list(l1, l2)\n",
    "    if len(l1)>len(l2):\n",
    "        s, l = l2, l1\n",
    "    elif len(l1)<len(l2):\n",
    "        s, l = l1, l2\n",
    "    else :\n",
    "        return l1, l2\n",
    "\n",
    "    s_len = len(s)\n",
    "    l_len = len(l)\n",
    "    ratio = l_len/s_len\n",
    "    keep_value = []\n",
    "    for j in range(s_len):\n",
    "        get_index = int(round((j+0.5)*ratio))\n",
    "        if(get_index >= l_len):\n",
    "            get_index = l_len - 1\n",
    "        keep_value.append(l[get_index])\n",
    "    return s, keep_value\n",
    "\n",
    "def remove_negative(series_1,series_2):\n",
    "    min_value = min(min(series_1), min(series_2))\n",
    "    if min_value<=0:\n",
    "        add_value=(abs(min_value)+1)\n",
    "        series_1 = [x+add_value for x in series_1]\n",
    "        series_2 = [x+add_value for x in series_2]\n",
    "    return series_1, series_2\n",
    "\n",
    "def KL_divergence_2(df1,df2):\n",
    "    from sklearn import metrics\n",
    "    return (metrics.mutual_info_score(df1,df2))\n",
    "\n",
    "def calculate_wasserstein_distance(series_1, series_2):\n",
    "    return wasserstein_distance(series_1, series_2)\n",
    "\n",
    "def drop_nans(df):\n",
    "    return df.dropna()\n",
    "\n",
    "def hellinger2(p, q):\n",
    "    import math\n",
    "    return sum([(math.sqrt(t[0])-math.sqrt(t[1]))*(math.sqrt(t[0])-math.sqrt(t[1]))\\\n",
    "                for t in zip(p,q)])/math.sqrt(2.)\n",
    "\n",
    "def ks_test(series_1, series_2):\n",
    "    return ks_2samp(series_1,series_2)\n",
    "\n",
    "def sorted_list(series_1,series_2):\n",
    "    sorted_1 = sorted(series_1)\n",
    "    sorted_2 = sorted(series_2)\n",
    "    return sorted_1, sorted_2\n",
    "\n",
    "def distance_metrics(df1,df2, column):\n",
    "    series_1, series_2 = basic_drop_series(df1[column],df2[column] )\n",
    "    \n",
    "    series_1,series_2 = same_length_lists(series_1,series_2)\n",
    "    series_1,series_2 = remove_negative(series_1,series_2)\n",
    "    \n",
    "    kl_divergence_result = KL_divergence_2(series_1, series_2)\n",
    "    wasserstein_distance_result = calculate_wasserstein_distance(series_1, series_2)\n",
    "    hellinger_distance_result = hellinger2(series_1, series_2)\n",
    "    ks_test_result = ks_test(series_1, series_2)[1]\n",
    "    return kl_divergence_result, wasserstein_distance_result, hellinger_distance_result, ks_test_result\n",
    "\n",
    "\n",
    "\n",
    "class model_results(object):\n",
    "    def __init__(self, model_name, r2, MAE, MSE, predicted_sum, count, mean, std, bins):\n",
    "        self.model_name = model_name\n",
    "        self.r2 = r2\n",
    "        self.MAE = MAE\n",
    "        self.MSE = MSE\n",
    "        self.predicted_sum = predicted_sum\n",
    "        self.std=std\n",
    "        self.mean=mean\n",
    "        self.count=count\n",
    "        self.bins=bins\n",
    "        \n",
    "class model_data(object):\n",
    "    def __init__(self, train_dataset, test_dataset):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.normalized = None\n",
    "        self.actual_sum = None\n",
    "        self.models=[]\n",
    "        #per variable\n",
    "        self.std_train=[]\n",
    "        self.mean_train=[]\n",
    "        self.count_train=[]\n",
    "        self.bins_train=[]\n",
    "        \n",
    "        self.std_test=[]\n",
    "        self.mean_test=[]\n",
    "        self.count_test=[]\n",
    "        self.bins_test=[]\n",
    "        \n",
    "        self.kl_divergence=[]\n",
    "        self.wasserstein_distance=[]\n",
    "        self.hellinger_distance=[]\n",
    "        self.ks_test=[]\n",
    "        \n",
    "    def add_model(self, model_name, r2, MAE, MSE, predicted_sum, count, mean, std, bins):\n",
    "        self.models.append(model_results(model_name, r2, MAE, MSE, predicted_sum, count, mean, std, bins))\n",
    "        \n",
    "    def print_model_details(self):\n",
    "        print('Train dataset: {} Test dataset: {}'.format(self.train_dataset, self.test_dataset))\n",
    "        print('Data normalization is {}'.format(self.normalized))\n",
    "        print('The target amount is {}'.format(self.actual_sum))\n",
    "        print('The following is a list of the models used and results obtained')\n",
    "        print('Format name'.ljust(25)+'\\t r2 \\t MSE \\t\\t MAE \\t predicted sum')\n",
    "        for model in self.models:\n",
    "            print('{}\\t{}\\t{}\\t{}\\t{}'.format(model.model_name.ljust(25),round(model.r2, 4), round(model.MSE,2), round(model.MAE,2), round(model.predicted_sum,2)))\n",
    "        \n",
    "    def output_to_csv(self,filename, features):\n",
    "        \n",
    "        new_model_outputs=[]\n",
    "        for model in self.models:\n",
    "            dictionary={'train_dataset': self.train_dataset, 'test_dataset':self.test_dataset, 'normalized':self.normalized,\n",
    "            'actual_sum': self.actual_sum, 'model_name':model.model_name, 'r2':model.r2, 'MSE': model.MSE,\n",
    "            'MAE':model.MAE, 'predicted_sum': model.predicted_sum, 'entries_amount': model.count, 'standard_deviation': model.std,\n",
    "            'mean': model.mean}\n",
    "            i=0\n",
    "            for bin_value in model.bins:\n",
    "                dictionary['model_bin_number_'+str(i)]=bin_value\n",
    "                i+=1\n",
    "            i=0\n",
    "            for feature in features:\n",
    "                dictionary[feature+'_std_train']=self.std_train[i]\n",
    "                dictionary[feature+'_mean_train']=self.mean_train[i]\n",
    "                dictionary[feature+'_count_train']=self.count_train[i]\n",
    "                \n",
    "                dictionary[feature+'_std_test']=self.std_test[i]\n",
    "                dictionary[feature+'_mean']=self.mean_test[i]\n",
    "                dictionary[feature+'_count']=self.count_test[i]\n",
    "                \n",
    "                dictionary[feature+'_kl_divergence']=self.kl_divergence[i]\n",
    "                dictionary[feature+'_wasserstein_distance']=self.wasserstein_distance[i]\n",
    "                dictionary[feature+'_hellinger_distance']=self.hellinger_distance[i]\n",
    "                dictionary[feature+'_ks_test']=self.ks_test[i]\n",
    "                \n",
    "                j=0\n",
    "                for bin_value in self.bins_train[i]:\n",
    "                    dictionary[feature+'_bin_number_train_'+str(j)]=bin_value\n",
    "                    j+=1\n",
    "                j=0\n",
    "                for bin_value in self.bins_test[i]:\n",
    "                    dictionary[feature+'_bin_number_test_'+str(j)]=bin_value\n",
    "                    j+=1\n",
    "                i+=1\n",
    "            new_model_outputs.append(dictionary)\n",
    "        df = pd.DataFrame(new_model_outputs)\n",
    "        \n",
    "        df.to_csv(filename, mode='a', header= (not os.path.exists(filename)))\n",
    "def get_name(string):\n",
    "    return string.rsplit('/',1)[1][0:-4]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ran\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_levels=[0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "noise_levels=[0.1]\n",
    "for noise_level in noise_levels:\n",
    "    \n",
    "    run_dataset = 'friedman_10_noise_'+str(noise_level)+'_window_size_2'\n",
    "    directory ='toy_datasets/'+run_dataset\n",
    "    result = glob.glob(directory +'/*.csv')\n",
    "    result.sort()\n",
    "    dataset_names=result\n",
    "    features =['0','1','2','3']\n",
    "    for train_dataset_name in dataset_names:\n",
    "        for target_dataset_name in dataset_names:\n",
    "            if train_dataset_name != target_dataset_name:\n",
    "                df_train = pd.read_csv(train_dataset_name)\n",
    "                df_target = pd.read_csv(target_dataset_name)\n",
    "\n",
    "                reporter_objects=[]\n",
    "                reporter_object=model_data(train_dataset_name,target_dataset_name)\n",
    "                for _ in range(1000):\n",
    "                    reporter_objects.append(model_data(get_name(train_dataset_name),get_name(target_dataset_name)))\n",
    "\n",
    "                i=0\n",
    "                apply_processing_posibilities=[False]\n",
    "\n",
    "                for apply_preprocessing in apply_processing_posibilities:\n",
    "                    for feature in features:\n",
    "\n",
    "                        count, mean, std, bins = split_into_bins(df_train, column=feature)\n",
    "                        reporter_objects[i].std_train.append(std)\n",
    "                        reporter_objects[i].mean_train.append(mean) \n",
    "                        reporter_objects[i].count_train.append(count)\n",
    "                        reporter_objects[i].bins_train.append(bins)\n",
    "\n",
    "                        count, mean, std, bins = split_into_bins(df_target, column=feature)\n",
    "                        reporter_objects[i].std_test.append(std)\n",
    "                        reporter_objects[i].mean_test.append(mean) \n",
    "                        reporter_objects[i].count_test.append(count)\n",
    "                        reporter_objects[i].bins_test.append(bins)\n",
    "\n",
    "                        kl_divergence, wasserstein_distance_result, hellinger_distance, ks_test_result = distance_metrics(df_train, df_target,column=feature)\n",
    "                        reporter_objects[i].kl_divergence.append(kl_divergence)\n",
    "                        reporter_objects[i].wasserstein_distance.append(wasserstein_distance_result)\n",
    "                        reporter_objects[i].hellinger_distance.append(hellinger_distance)\n",
    "                        reporter_objects[i].ks_test.append(ks_test_result)\n",
    "\n",
    "\n",
    "                    reporter_objects[i] = run_datasets(df_train, df_target, target = 'target', columns=features,apply_preprocessing=apply_preprocessing, reporter_object=reporter_objects[i])\n",
    "                    #reporter_objects[i].print_model_details()\n",
    "                    i+=1\n",
    "                for j in range(i):\n",
    "                    if not os.path.exists('toy_datasets/ran_datasets/'):\n",
    "                        os.makedirs('toy_datasets/ran_datasets/')\n",
    "                    #reporter_objects[j].print_model_details()\n",
    "                    reporter_objects[j].output_to_csv('toy_datasets/ran_datasets/'+run_dataset+'.csv',features)\n",
    "    print('Models ran')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
